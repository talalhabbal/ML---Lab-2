{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:585: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.object,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GRU, Dense\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\__init__.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\__init__.py:46\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:97\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:353\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m--> 353\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute_options\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoShardPolicy\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute_options\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExternalStatePolicy\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompress\u001b[39m(element):\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwrapt\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:40\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m _sparse_tensor\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_callbacks\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:585\u001b[0m\n\u001b[0;32m    556\u001b[0m     _NP_TO_TF[pdt] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m    557\u001b[0m         _NP_TO_TF[dt] \u001b[38;5;28;01mfor\u001b[39;00m dt \u001b[38;5;129;01min\u001b[39;00m _NP_TO_TF \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m==\u001b[39m pdt()\u001b[38;5;241m.\u001b[39mdtype)  \u001b[38;5;66;03m# pylint: disable=no-value-for-parameter\u001b[39;00m\n\u001b[0;32m    559\u001b[0m TF_VALUE_DTYPES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(_NP_TO_TF\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    561\u001b[0m _TF_TO_NP \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    562\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_HALF:\n\u001b[0;32m    563\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m    564\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_FLOAT:\n\u001b[0;32m    565\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m    566\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_DOUBLE:\n\u001b[0;32m    567\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m    568\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT32:\n\u001b[0;32m    569\u001b[0m         np\u001b[38;5;241m.\u001b[39mint32,\n\u001b[0;32m    570\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT8:\n\u001b[0;32m    571\u001b[0m         np\u001b[38;5;241m.\u001b[39muint8,\n\u001b[0;32m    572\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT16:\n\u001b[0;32m    573\u001b[0m         np\u001b[38;5;241m.\u001b[39muint16,\n\u001b[0;32m    574\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT32:\n\u001b[0;32m    575\u001b[0m         np\u001b[38;5;241m.\u001b[39muint32,\n\u001b[0;32m    576\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT64:\n\u001b[0;32m    577\u001b[0m         np\u001b[38;5;241m.\u001b[39muint64,\n\u001b[0;32m    578\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT16:\n\u001b[0;32m    579\u001b[0m         np\u001b[38;5;241m.\u001b[39mint16,\n\u001b[0;32m    580\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT8:\n\u001b[0;32m    581\u001b[0m         np\u001b[38;5;241m.\u001b[39mint8,\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;66;03m# NOTE(touts): For strings we use np.object as it supports variable length\u001b[39;00m\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# strings.\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_STRING:\n\u001b[1;32m--> 585\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m,\n\u001b[0;32m    586\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX64:\n\u001b[0;32m    587\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex64,\n\u001b[0;32m    588\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX128:\n\u001b[0;32m    589\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex128,\n\u001b[0;32m    590\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT64:\n\u001b[0;32m    591\u001b[0m         np\u001b[38;5;241m.\u001b[39mint64,\n\u001b[0;32m    592\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BOOL:\n\u001b[0;32m    593\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[0;32m    594\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT8:\n\u001b[0;32m    595\u001b[0m         _np_qint8,\n\u001b[0;32m    596\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT8:\n\u001b[0;32m    597\u001b[0m         _np_quint8,\n\u001b[0;32m    598\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT16:\n\u001b[0;32m    599\u001b[0m         _np_qint16,\n\u001b[0;32m    600\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT16:\n\u001b[0;32m    601\u001b[0m         _np_quint16,\n\u001b[0;32m    602\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT32:\n\u001b[0;32m    603\u001b[0m         _np_qint32,\n\u001b[0;32m    604\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BFLOAT16:\n\u001b[0;32m    605\u001b[0m         _np_bfloat16,\n\u001b[0;32m    606\u001b[0m \n\u001b[0;32m    607\u001b[0m     \u001b[38;5;66;03m# Ref types\u001b[39;00m\n\u001b[0;32m    608\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_HALF_REF:\n\u001b[0;32m    609\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m    610\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_FLOAT_REF:\n\u001b[0;32m    611\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m    612\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_DOUBLE_REF:\n\u001b[0;32m    613\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m    614\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT32_REF:\n\u001b[0;32m    615\u001b[0m         np\u001b[38;5;241m.\u001b[39mint32,\n\u001b[0;32m    616\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT32_REF:\n\u001b[0;32m    617\u001b[0m         np\u001b[38;5;241m.\u001b[39muint32,\n\u001b[0;32m    618\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT8_REF:\n\u001b[0;32m    619\u001b[0m         np\u001b[38;5;241m.\u001b[39muint8,\n\u001b[0;32m    620\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT16_REF:\n\u001b[0;32m    621\u001b[0m         np\u001b[38;5;241m.\u001b[39muint16,\n\u001b[0;32m    622\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT16_REF:\n\u001b[0;32m    623\u001b[0m         np\u001b[38;5;241m.\u001b[39mint16,\n\u001b[0;32m    624\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT8_REF:\n\u001b[0;32m    625\u001b[0m         np\u001b[38;5;241m.\u001b[39mint8,\n\u001b[0;32m    626\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_STRING_REF:\n\u001b[0;32m    627\u001b[0m         np\u001b[38;5;241m.\u001b[39mobject,\n\u001b[0;32m    628\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX64_REF:\n\u001b[0;32m    629\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex64,\n\u001b[0;32m    630\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX128_REF:\n\u001b[0;32m    631\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex128,\n\u001b[0;32m    632\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT64_REF:\n\u001b[0;32m    633\u001b[0m         np\u001b[38;5;241m.\u001b[39mint64,\n\u001b[0;32m    634\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT64_REF:\n\u001b[0;32m    635\u001b[0m         np\u001b[38;5;241m.\u001b[39muint64,\n\u001b[0;32m    636\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BOOL_REF:\n\u001b[0;32m    637\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool,\n\u001b[0;32m    638\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT8_REF:\n\u001b[0;32m    639\u001b[0m         _np_qint8,\n\u001b[0;32m    640\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT8_REF:\n\u001b[0;32m    641\u001b[0m         _np_quint8,\n\u001b[0;32m    642\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT16_REF:\n\u001b[0;32m    643\u001b[0m         _np_qint16,\n\u001b[0;32m    644\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT16_REF:\n\u001b[0;32m    645\u001b[0m         _np_quint16,\n\u001b[0;32m    646\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT32_REF:\n\u001b[0;32m    647\u001b[0m         _np_qint32,\n\u001b[0;32m    648\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BFLOAT16_REF:\n\u001b[0;32m    649\u001b[0m         _np_bfloat16,\n\u001b[0;32m    650\u001b[0m }\n\u001b[0;32m    652\u001b[0m _QUANTIZED_DTYPES_NO_REF \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([qint8, quint8, qint16, quint16, qint32])\n\u001b[0;32m    653\u001b[0m _QUANTIZED_DTYPES_REF \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(\n\u001b[0;32m    654\u001b[0m     [qint8_ref, quint8_ref, qint16_ref, quint16_ref, qint32_ref])\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Lab1\\lib\\site-packages\\numpy\\__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_unpickle(class_type, directory, subdirectory):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Loads and Unpickles a file from the supplied parameters\n",
    "    \n",
    "    Parameter:\n",
    "    class_type (str): class type to be loaded and unpickled\\n\n",
    "    directory (str): directory within the CWD\\n\n",
    "    subdirectory (str): subdirectory within the directory specified\\n\n",
    "    \n",
    "    Returns:\n",
    "    Specified dataframe if found in the directory\\n\n",
    "    None if not found\\n\n",
    "    \"\"\"\n",
    "    dir_path = f\"{directory}/{subdirectory}\"\n",
    "    filename = f'{class_type}.pkl'\n",
    "    filepath = os.path.join(dir_path, filename)\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        df = pd.read_pickle(filepath)\n",
    "        print(f\"Loaded from {subdirectory} for class {class_type}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframes(clean=False):\n",
    "    classes = [\"Jump\", \"Run\", \"Walk\", \"Squat\"]\n",
    "    original_directory = \"Original\"\n",
    "    clean_directory = \"Clean\"\n",
    "    training_subdirectory = \"Training\"\n",
    "    test_subdirectory = \"Test\"\n",
    "    validation_subdirectory = \"Validation\"\n",
    "    \n",
    "    train_df_arr = []\n",
    "    test_df_arr = []\n",
    "    valid_df_arr = []\n",
    "    \n",
    "    complete_train_df = pd.DataFrame()\n",
    "    complete_test_df = pd.DataFrame()\n",
    "    complete_valid_df = pd.DataFrame()\n",
    "    \n",
    "    for class_type in classes:\n",
    "        if(clean):\n",
    "            train_df_arr.append(load_and_unpickle(class_type, clean_directory, training_subdirectory))\n",
    "            test_df_arr.append(load_and_unpickle(class_type, clean_directory, test_subdirectory))\n",
    "            valid_df_arr.append(load_and_unpickle(class_type, clean_directory, validation_subdirectory))\n",
    "        else:\n",
    "            train_df_arr.append(load_and_unpickle(class_type, original_directory, training_subdirectory))\n",
    "            test_df_arr.append(load_and_unpickle(class_type, original_directory, test_subdirectory))\n",
    "            valid_df_arr.append(load_and_unpickle(class_type, original_directory, validation_subdirectory))\n",
    "    \n",
    "    for train_df, test_df, valid_df in zip(train_df_arr, test_df_arr, valid_df_arr):\n",
    "        complete_train_df = pd.concat([complete_train_df, train_df], ignore_index=True)\n",
    "        complete_test_df = pd.concat([complete_test_df, test_df], ignore_index=True)\n",
    "        complete_valid_df = pd.concat([complete_valid_df, valid_df], ignore_index=True)\n",
    "\n",
    "    return complete_train_df, complete_test_df, complete_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from Training for class Jump\n",
      "Loaded from Test for class Jump\n",
      "Loaded from Validation for class Jump\n",
      "Loaded from Training for class Run\n",
      "Loaded from Test for class Run\n",
      "Loaded from Validation for class Run\n",
      "Loaded from Training for class Walk\n",
      "Loaded from Test for class Walk\n",
      "Loaded from Validation for class Walk\n",
      "Loaded from Training for class Squat\n",
      "Loaded from Test for class Squat\n",
      "Loaded from Validation for class Squat\n",
      "Loaded from Training for class Jump\n",
      "Loaded from Test for class Jump\n",
      "Loaded from Validation for class Jump\n",
      "Loaded from Training for class Run\n",
      "Loaded from Test for class Run\n",
      "Loaded from Validation for class Run\n",
      "Loaded from Training for class Walk\n",
      "Loaded from Test for class Walk\n",
      "Loaded from Validation for class Walk\n",
      "Loaded from Training for class Squat\n",
      "Loaded from Test for class Squat\n",
      "Loaded from Validation for class Squat\n",
      "         ax        ay        az        gx        gy        gz Class\n",
      "0  0.103585 -9.798624 -0.484243 -0.017339 -0.057202  0.023274  Jump\n",
      "1  0.101190 -9.762100 -0.451461 -0.016421 -0.072191  0.024990  Jump\n",
      "2  0.082329 -9.745185 -0.364043 -0.000619 -0.088786  0.027404  Jump\n",
      "3  0.032632 -9.745484 -0.268691  0.023103 -0.102205  0.027474  Jump\n",
      "4 -0.012724 -9.780511 -0.205523  0.041492 -0.114810  0.028797  Jump\n",
      "             ax        ay        az        gx        gy        gz  Class\n",
      "18130  3.478766  4.755012 -9.366772 -0.933489 -0.655683 -0.701356  Squat\n",
      "18131  2.947820  4.628525 -9.447753 -0.903371 -0.572386 -0.692968  Squat\n",
      "18132  2.467469  4.418063 -9.746083 -0.858307 -0.531870 -0.668633  Squat\n",
      "18133  2.075135  4.488117 -9.616303 -0.867265 -0.471442 -0.638825  Squat\n",
      "18134  1.797163  4.720284 -9.850117 -0.834673 -0.500778 -0.579327  Squat\n"
     ]
    }
   ],
   "source": [
    "original_train_df, original_test_df, original_valid_df = get_dataframes()\n",
    "clean_train_df, clean_test_df, clean_valid_df = get_dataframes(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_neural_network(neurons, hidden_layers, timesteps):\n",
    "    model = Sequential()\n",
    "    # Input Layer with first hidden layer\n",
    "    model.add(GRU(neurons, input_shape=(timesteps, 6), activation='tanh', return_sequences=True, name=\"Hidden_Layer_1_GRU\"))\n",
    "    # Add Hidden layers\n",
    "    for i in range(0, hidden_layers - 1):\n",
    "        model.add(GRU(neurons, activation='tanh', name=f\"Hidden_layer_{i+2}_GRU\", return_sequences=True))\n",
    "   \n",
    "    model.add(GRU(neurons, activation='tanh', name=f\"Hidden_layer_{hidden_layers+1}_GRU\"))\n",
    "    # Output Layer\n",
    "    model.add(Dense(4, activation='softmax', name=\"Output_Layer\"))\n",
    "    # Compile Model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_gru_model(train_df, valid_df, neurons, hidden_layers, timesteps, batch_size=32, epochs=50, normalize=False):\n",
    "#         # Prepare features and labels\n",
    "#     X_train = train_df[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].values  # Feature columns\n",
    "#     Y_train = train_df['Class'].values  # Label column\n",
    "#     X_val = valid_df[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].values # Feature columns\n",
    "#     Y_val = valid_df['Class'].values  # Label column\n",
    "    \n",
    "#     # Normalize data if required\n",
    "#     if normalize:\n",
    "#         scaler = MinMaxScaler()\n",
    "#         X_train = scaler.fit_transform(X_train)\n",
    "#         X_val = scaler.transform(X_val)\n",
    "#     # Reshape input data to be 3D [samples, timesteps, features]\n",
    "#     # Here, we assume that you want to create a sequence of 'timesteps' for each sample\n",
    "#     num_samples = X_train.shape[0] // timesteps  # Number of complete sequences\n",
    "#     X_train = X_train[:num_samples * timesteps].reshape(num_samples, timesteps, 6)  # Reshape to 3D\n",
    "#     Y_train = Y_train[: num_samples * timesteps]  # Corresponding labels\n",
    "\n",
    "#     num_val_samples = X_val.shape[0] // timesteps  # Number of complete validation sequences\n",
    "#     X_val = X_val[:num_val_samples * timesteps].reshape(num_val_samples, timesteps, 6)  # Reshape to 3D\n",
    "#     Y_val = Y_val[: num_val_samples * timesteps]  # Corresponding validation labels\n",
    "#     # Encode labels\n",
    "#     label_encoder = LabelEncoder()\n",
    "#     Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "#     Y_val_encoded = label_encoder.transform(Y_val)\n",
    "    \n",
    "#     # Create GRU model\n",
    "#     model = create_gru_neural_network(neurons=neurons, hidden_layers=hidden_layers, timesteps=timesteps)\n",
    "    \n",
    "#     # Train the model\n",
    "#     history = model.fit(\n",
    "#         x=X_train,  # Reshape for GRU input\n",
    "#         y=Y_train_encoded,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=epochs,\n",
    "#         verbose=1,\n",
    "#         validation_data=(X_val, Y_val_encoded)\n",
    "#     )\n",
    "    \n",
    "#     return model, history, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gru_model(train_df, valid_df, neurons, hidden_layers, timesteps, batch_size=32, epochs=50, normalize=False):\n",
    "    # Prepare features and labels\n",
    "    X_train = train_df[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].values  # Feature columns\n",
    "    Y_train = train_df['Class'].values  # Label column\n",
    "    X_val = valid_df[['ax', 'ay', 'az', 'gx', 'gy', 'gz']].values # Feature columns\n",
    "    Y_val = valid_df['Class'].values  # Label column\n",
    "\n",
    "    # Normalize data if required\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Reshape input data to be 3D [samples, timesteps, features]\n",
    "    num_samples = X_train.shape[0] // timesteps  # Number of complete sequences\n",
    "    X_train = X_train[:num_samples * timesteps].reshape(num_samples, timesteps, 6)  # Reshape to 3D\n",
    "    Y_train = Y_train[:num_samples * timesteps]  # Corresponding labels\n",
    "    \n",
    "    # Make sure Y_train has the same number of entries as X_train's samples\n",
    "    Y_train = Y_train[:num_samples]  # Adjust to only the number of sequences\n",
    "\n",
    "    num_val_samples = X_val.shape[0] // timesteps  # Number of complete validation sequences\n",
    "    X_val = X_val[:num_val_samples * timesteps].reshape(num_val_samples, timesteps, 6)  # Reshape to 3D\n",
    "    Y_val = Y_val[:num_val_samples * timesteps]  # Corresponding validation labels\n",
    "\n",
    "    # Make sure Y_val has the same number of entries as X_val's samples\n",
    "    Y_val = Y_val[:num_val_samples]  # Adjust to only the number of sequences\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "    Y_val_encoded = label_encoder.transform(Y_val)\n",
    "\n",
    "    # Create GRU model\n",
    "    model = create_gru_neural_network(neurons=neurons, hidden_layers=hidden_layers, timesteps=timesteps)\n",
    "    # # Train the model\n",
    "    with tf.device('/GPU:0'):\n",
    "        history = model.fit(\n",
    "            x=X_train,  # Reshape for GRU input\n",
    "            y=Y_train_encoded,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(X_val, Y_val_encoded)\n",
    "        )\n",
    "    \n",
    "    return model, history, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model, history, label_encoder \u001b[38;5;241m=\u001b[39m train_gru_model(\n\u001b[1;32m----> 2\u001b[0m     train_df\u001b[38;5;241m=\u001b[39m\u001b[43moriginal_train_df\u001b[49m,\n\u001b[0;32m      3\u001b[0m     valid_df\u001b[38;5;241m=\u001b[39moriginal_valid_df,\n\u001b[0;32m      4\u001b[0m     neurons\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      5\u001b[0m     hidden_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      6\u001b[0m     timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m\n\u001b[0;32m      7\u001b[0m     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'original_train_df' is not defined"
     ]
    }
   ],
   "source": [
    "model, history, label_encoder = train_gru_model(\n",
    "    train_df=original_train_df,\n",
    "    valid_df=original_valid_df,\n",
    "    neurons=10,\n",
    "    hidden_layers=1,\n",
    "    timesteps=300\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
